{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ecd1b7",
   "metadata": {},
   "source": [
    "## Memory - Context in Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d14d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: /home/hp/Desktop/Workplace/CustomizeGPT/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 39ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-core langgraph langgraph-checkpoint-postgres psycopg[binary,pool] openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed930e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: /home/hp/Desktop/Workplace/CustomizeGPT/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m35 packages\u001b[0m \u001b[2min 2.44s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)                                                  \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)-------------\u001b[0m\u001b[0m     0 B/64.96 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 16.00 KiB/64.96 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 16.00 KiB/64.96 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 32.00 KiB/64.96 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB         \u001b[1A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m     0 B/267.30 KiB          \u001b[2A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m     0 B/267.30 KiB          \u001b[2A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m     0 B/267.30 KiB          \u001b[2A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m     0 B/466.19 KiB          \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.33 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m     0 B/466.19 KiB          \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 40.72 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.33 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 13.14 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 56.72 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 4.33 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 13.14 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 56.72 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 13.14 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 56.72 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 14.88 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.96 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 14.88 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.96 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 14.88 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlanggraph-sdk       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.96 KiB/64.96 KiB\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 14.88 KiB/466.19 KiB        \u001b[3A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 14.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 30.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 14.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 30.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 30.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 46.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 30.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 46.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 46.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 62.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 46.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 62.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 62.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 78.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 62.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 78.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/13)------------\u001b[0m\u001b[0m 78.78 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 78.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/13)------------\u001b[0m\u001b[0m 78.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 78.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/13)------------\u001b[0m\u001b[0m 78.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 94.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/13)------------\u001b[0m\u001b[0m 78.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 142.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/13)------------\u001b[0m\u001b[0m 94.88 KiB/466.19 KiB        \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 206.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/13)------------\u001b[0m\u001b[0m 174.88 KiB/466.19 KiB       \u001b[2A\n",
      "\u001b[2mlangsmith           \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 222.92 KiB/267.30 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/13)------------\u001b[0m\u001b[0m 222.88 KiB/466.19 KiB       \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)------------\u001b[0m\u001b[0m 254.88 KiB/466.19 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m13 packages\u001b[0m \u001b[2min 821ms\u001b[0m\u001b[0m                                                \u001b[1A\n",
      "\u001b[2mUninstalled \u001b[1m13 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.10.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.3.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.38\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.5.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.12.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be549b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SUPABASE_DB_URI = os.getenv(\"SUPABASE_DB_URI\")\n",
    "# print(SUPABASE_DB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b6d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp/Desktop/Workplace/CustomizeGPT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.postgres import PostgresSaver  \n",
    "\n",
    "model = init_chat_model(model=\"gpt-4o-mini\")\n",
    "\n",
    "with PostgresSaver.from_conn_string(SUPABASE_DB_URI) as checkpointer:  \n",
    "    checkpointer.setup()\n",
    "\n",
    "    def call_model(state: MessagesState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "\n",
    "    graph = builder.compile(checkpointer=checkpointer)  \n",
    "\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for chunk in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,  \n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    for chunk in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,  \n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742d36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.get(\"/conversations/{conversation_id}\")\n",
    "async def get_conversation_history(conversation_id: str):\n",
    "    \"\"\"\n",
    "    Retrieve the message history for a specific conversation (thread) from Postgres.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configuration for the checkpointer\n",
    "        config = {\"configurable\": {\"thread_id\": conversation_id}}\n",
    "        \n",
    "        with PostgresSaver.from_conn_string(SUPABASE_DB_URI) as checkpointer:\n",
    "            # Fetch the latest state checkpoint for this thread\n",
    "            state = checkpointer.get_tuple(config)\n",
    "            \n",
    "            if not state:\n",
    "                return {\"messages\": [], \"message\": \"No history found for this thread.\"}\n",
    "\n",
    "            # Extract the 'messages' channel from the state values\n",
    "            # Note: This depends on your State definition (usually 'messages')\n",
    "            checkpoint_values = state.checkpoint.get(\"channel_values\", {})\n",
    "            messages = checkpoint_values.get(\"messages\", [])\n",
    "\n",
    "            return {\n",
    "                \"thread_id\": conversation_id,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        # raise HTTPException(status_code=500, detail=f\"Failed to retrieve conversation: {str(e)}\")\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6131ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thread_id': '1', 'messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='957052f1-447d-4a31-b0f6-a73355209844'), AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CpW78T5XSw26WxV4tcQXqZuQbYODd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b454c-9f9a-79d3-af28-f50fff30a87f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='ad4735de-b105-4abe-92a3-aeab7ef23995'), AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 34, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CpW7B612MZw93195pYsYpkPixPi3G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b454c-acf7-7a63-8a5c-d8b7a2d2cdde-0', usage_metadata={'input_tokens': 34, 'output_tokens': 14, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34692/2444764511.py:1: RuntimeWarning: coroutine 'get_conversation_history' was never awaited\n",
      "  response = await get_conversation_history(\"1\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "response = await get_conversation_history(\"1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd96542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'human', 'content': \"hi! I'm bob\"}, {'role': 'ai', 'content': 'Hi Bob! How can I assist you today?'}, {'role': 'human', 'content': \"what's my name?\"}, {'role': 'ai', 'content': 'Your name is Bob! How can I help you today, Bob?'}]\n"
     ]
    }
   ],
   "source": [
    "formatted_messages = [\n",
    "    {\"role\": msg.type, \"content\": msg.content} for msg in response[\"messages\"]\n",
    "]\n",
    "\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2547d",
   "metadata": {},
   "source": [
    "## DPR + Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1969c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ac6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch, faiss, numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec08b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "ctx_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
    "\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
    "\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPRVectorStore:\n",
    "    def __init__(self, ctx_model, q_model, ctx_tokenizer, q_tokenizer):\n",
    "        self.ctx_model = ctx_model\n",
    "        self.q_model = q_model\n",
    "        self.ctx_tokenizer = ctx_tokenizer\n",
    "        self.q_tokenizer = q_tokenizer\n",
    "        self.index = None\n",
    "        self.passages = []\n",
    "        self.embeddings = None\n",
    "\n",
    "    def build_index(self, passages):\n",
    "        self.passages = passages\n",
    "        inputs = self.ctx_tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            self.embeddings = self.ctx_model(**inputs.to(device)).pooler_output.cpu().numpy()\n",
    "        dim = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(self.embeddings)\n",
    "\n",
    "    def query(self, question, top_k=5):\n",
    "        q_inputs = self.q_tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            q_embed = self.q_model(**q_inputs).pooler_output.cpu().numpy()\n",
    "        D, I = self.index.search(q_embed, top_k)\n",
    "        results = [self.passages[i] for i in I[0]]\n",
    "        return results\n",
    "\n",
    "    def rerank(self, question, passages):\n",
    "        pairs = [(question, p) for p in passages]\n",
    "        scores = cross_encoder_model.predict(pairs)\n",
    "        ranked = [p for _, p in sorted(zip(scores, passages), reverse=True)]\n",
    "        return ranked, scores\n",
    "\n",
    "    def save_index(self, path=\"dpr_index.faiss\"):\n",
    "        faiss.write_index(self.index, path)\n",
    "\n",
    "    def load_index(self, path=\"dpr_index.faiss\"):\n",
    "        self.index = faiss.read_index(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849001ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 19\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/Sales/Case Studies/Aura Health.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 19 chunks\n"
     ]
    }
   ],
   "source": [
    "retriever = DPRVectorStore(ctx_model, q_model, ctx_tokenizer, q_tokenizer)\n",
    "retriever.build_index([c.page_content for c in chunks])\n",
    "print(\"Index built with\", len(chunks), \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55772e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPR Top-k Results:\n",
      "1. Emotion Detection Gap\n",
      "Therapists and wellness coaches needed better tools to create, \n",
      "publish, and monetize their content, but the platform lacked an \n",
      "intuitive, scalable interface to support growing creator needs.\n",
      "Creator Experience Limitations\n",
      "Aura’s expansion into workplaces required robust infrastructure to \n",
      "handle enterprise-scale usage, real-time personalization, and\n",
      "integration with collaboration tools all without sacriﬁcing\n",
      "performance or privacy.\n",
      "Scalability for Enterprise Use\n",
      "2. Impact\n",
      "Emotion AI delivered a 40% increase in content relevance, as users \n",
      "engaged more with mood-matched meditations, stories, and\n",
      "therapy sessions.\n",
      "Real-Time Personalization\n",
      "Upgraded UI/UX and personalized content journeys led to a 35% \n",
      "uplift in daily active users and a 25% increase in session duration \n",
      "across platforms.\n",
      "Boost in User Engagement\n",
      "The creator tools resulted in a 60% increase in audio content \n",
      "ploads and a 3x growth in active wellness coaches contributing \n",
      "to the platform.\n",
      "3. Developed and deployed AI models that detect emotional \n",
      "cues from user interactions in Slack, Zoom, and the\n",
      "mobile/web app, enabling dynamic content recommenda-\n",
      "tions tailored to users’ real-time mental states.\n",
      "Emotion Intelligence Integration\n",
      "Implemented a new design framework with consistent UI \n",
      "components across web and mobile, improving usability, \n",
      "accessibility, and responsiveness across all user journeys.\n",
      "Design System Modernization\n",
      "Refactored the platform’s front-end and back-end layers\n",
      "4. one that provides healing anytime, anywhere. At the heart of this vision \n",
      "is Aura’s Emotion AI, a powerful capability that reads user emotions \n",
      "from their interactions across Slack, Zoom, and other tools, then\n",
      "recommends tailored content such as guided meditations, sleep\n",
      "stories, or cognitive therapy sessions.Through its extensive wellness \n",
      "marketplace, the platform empowers certiﬁed coaches and therapists \n",
      "to publish audio content, build supportive communities, and offer\n",
      "5. coaching—directly enriching Aura’s marketplace and\n",
      "increasing platform engagement.\n",
      "Creator Enablement Tools\n",
      "Designed scalable infrastructure for the growing wellness \n",
      "marketplace, supporting new categories, regional content \n",
      "ﬁlters, and performance analytics for creators.\n",
      "Marketplace Ecosystem Expansion\n",
      "Ensured HIPAA and GDPR alignment through secure data \n",
      "handling, consent-based emotion tracking, and\n",
      "anonymized insights protecting user privacy while enabling \n",
      "meaningful personalization.\n",
      "\n",
      "Cross-Encoder Reranked Results:\n",
      "1. Developed and deployed AI models that detect emotional \n",
      "cues from user interactions in Slack, Zoom, and the\n",
      "mobile/web app, enabling dynamic content recommenda-\n",
      "tions tailored to users’ real-time mental states.\n",
      "Emotion Intelligence Integration\n",
      "Implemented a new design framework with consistent UI \n",
      "components across web and mobile, improving usability, \n",
      "accessibility, and responsiveness across all user journeys.\n",
      "Design System Modernization\n",
      "Refactored the platform’s front-end and back-end layers (score: -3.5556)\n",
      "2. one that provides healing anytime, anywhere. At the heart of this vision \n",
      "is Aura’s Emotion AI, a powerful capability that reads user emotions \n",
      "from their interactions across Slack, Zoom, and other tools, then\n",
      "recommends tailored content such as guided meditations, sleep\n",
      "stories, or cognitive therapy sessions.Through its extensive wellness \n",
      "marketplace, the platform empowers certiﬁed coaches and therapists \n",
      "to publish audio content, build supportive communities, and offer (score: -5.6256)\n",
      "3. Emotion Detection Gap\n",
      "Therapists and wellness coaches needed better tools to create, \n",
      "publish, and monetize their content, but the platform lacked an \n",
      "intuitive, scalable interface to support growing creator needs.\n",
      "Creator Experience Limitations\n",
      "Aura’s expansion into workplaces required robust infrastructure to \n",
      "handle enterprise-scale usage, real-time personalization, and\n",
      "integration with collaboration tools all without sacriﬁcing\n",
      "performance or privacy.\n",
      "Scalability for Enterprise Use (score: 7.5982)\n",
      "4. Impact\n",
      "Emotion AI delivered a 40% increase in content relevance, as users \n",
      "engaged more with mood-matched meditations, stories, and\n",
      "therapy sessions.\n",
      "Real-Time Personalization\n",
      "Upgraded UI/UX and personalized content journeys led to a 35% \n",
      "uplift in daily active users and a 25% increase in session duration \n",
      "across platforms.\n",
      "Boost in User Engagement\n",
      "The creator tools resulted in a 60% increase in audio content \n",
      "ploads and a 3x growth in active wellness coaches contributing \n",
      "to the platform. (score: -3.5158)\n",
      "5. coaching—directly enriching Aura’s marketplace and\n",
      "increasing platform engagement.\n",
      "Creator Enablement Tools\n",
      "Designed scalable infrastructure for the growing wellness \n",
      "marketplace, supporting new categories, regional content \n",
      "ﬁlters, and performance analytics for creators.\n",
      "Marketplace Ecosystem Expansion\n",
      "Ensured HIPAA and GDPR alignment through secure data \n",
      "handling, consent-based emotion tracking, and\n",
      "anonymized insights protecting user privacy while enabling \n",
      "meaningful personalization. (score: -6.9207)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Emotion Intelligence Integration?\"\n",
    "\n",
    "dpr_results = retriever.query(query, top_k=5)\n",
    "reranked, scores = retriever.rerank(query, dpr_results)\n",
    "\n",
    "print(\"DPR Top-k Results:\")\n",
    "for i, p in enumerate(dpr_results, 1):\n",
    "    print(f\"{i}. {p}\")\n",
    "\n",
    "print(\"\\nCross-Encoder Reranked Results:\")\n",
    "for i, (p, s) in enumerate(zip(reranked, scores), 1):\n",
    "    print(f\"{i}. {p} (score: {s:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9e27137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrieve_documents(query: str):\n",
    "    \"\"\"Retrieve relevant documents.\"\"\"\n",
    "    dpr_results = retriever.query(query, top_k=3)\n",
    "    reranked, scores = retriever.rerank(query, dpr_results)\n",
    "\n",
    "    return reranked\n",
    "\n",
    "tools = [retrieve_documents]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a professional email assistant for our company's sales team. Your role is to respond to customer inquiries using ONLY information from our knowledge base.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. You MUST respond in proper business email format with subject line, salutation, body, and signature\n",
    "2. If the customer's question can be answered using the provided context, write a helpful, professional email response\n",
    "3. If the information is NOT in the knowledge base (context shows \"NO_RELEVANT_INFORMATION_FOUND\"), respond with a polite email explaining this\n",
    "4. Never invent information or use external knowledge\n",
    "5. Maintain a professional, helpful tone in all communications\n",
    "6. Format your response as a ready-to-send email\n",
    "7. Always start the subject with \"Re: \" followed by the original subject or an appropriate title\n",
    "\n",
    "EMAIL FORMAT:\n",
    "Subject: Re: [Original Subject or Appropriate Title]\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "[Professional email body acknowledging their query and providing information or explaining limitations]\n",
    "\n",
    "[Clear next steps or contact information if needed]\n",
    "\n",
    "Best regards,\n",
    "[Sale Team]\n",
    "[Strategisthub]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.graph import START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                        content=EMAIL_SYSTEM_PROMPT\n",
    "                    )\n",
    "        ]\n",
    "        + state[\"messages\"]\n",
    "        )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# 7. Build graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# app = workflow.compile()\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5226cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Inquiry About Emotion Intelligence Integration\n",
      "\n",
      "Dear [Customer Name],\n",
      "\n",
      "Thank you for your inquiry regarding Emotion Intelligence Integration. \n",
      "\n",
      "Emotion Intelligence Integration involves the development and deployment of AI models that detect emotional cues from user interactions across various platforms such as Slack, Zoom, and mobile/web applications. This technology enables dynamic content recommendations tailored to users' real-time mental states, enhancing user engagement and content relevance.\n",
      "\n",
      "For instance, the implementation of this integration has led to a 40% increase in content relevance, as users engage more with mood-matched meditations, stories, and therapy sessions. Additionally, it has contributed to a significant boost in user engagement metrics.\n",
      "\n",
      "If you have any further questions or need more detailed information, please feel free to reach out.\n",
      "\n",
      "Best regards,  \n",
      "Sales Team  \n",
      "Strategisthub\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "response = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2f41658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Inquiry About Aura Health\n",
      "\n",
      "Dear [Customer Name],\n",
      "\n",
      "Thank you for your inquiry about Aura Health.\n",
      "\n",
      "Aura Health is a globally recognized leader in mental wellness, trusted by over 8 million users. It has been honored with several awards, including Apple’s Best of Apps and the Very Well Mind Online Therapy and Wellness Award. Aura provides a category-defining platform in digital wellness, combining the expertise of the world’s best coaches and therapists to deliver a deeply personalized self-care journey for each user.\n",
      "\n",
      "The platform's vision is to create a global digital ecosystem for mental health, offering healing anytime and anywhere. A key feature of Aura is its Emotion AI, which reads user emotions from their interactions across various tools and recommends tailored content such as guided meditations, sleep stories, and cognitive therapy sessions.\n",
      "\n",
      "If you have any further questions or need additional information, please feel free to reach out.\n",
      "\n",
      "Best regards,  \n",
      "Sales Team  \n",
      "Strategisthub\n"
     ]
    }
   ],
   "source": [
    "query = \"What is aura health?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "response = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53da031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e69954e4",
   "metadata": {},
   "source": [
    "# Testing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dba54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CustomizeGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
